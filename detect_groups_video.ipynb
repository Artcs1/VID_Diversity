{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa51d744-56e7-4147-9691-3892feb50f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import math\n",
    "import time\n",
    "import requests\n",
    "import torch\n",
    "import os\n",
    "import torchreid\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "import shutil\n",
    "import logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from ultralytics import YOLO\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from our_utils import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    spacing = 10\n",
    "    eval_frames = 100\n",
    "    \n",
    "    video_metadata = {}\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    qwen = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\",torch_dtype=torch.bfloat16,attn_implementation=\"flash_attention_2\",device_map=\"cuda:0\",)\n",
    "    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "    qwen.eval()\n",
    "    \n",
    "    pose = YOLO(\"yolo11m-pose.pt\", 0.15)\n",
    "    model = YOLO(\"yolov10m.pt\")\n",
    "    tracker = sv.ByteTrack(track_activation_threshold=0.1)\n",
    "    palette = sv.ColorPalette.DEFAULT\n",
    "    \n",
    "    #cap = cv2.VideoCapture(\"input.mp4\")\n",
    "    #video_path = \"/gpfs/projects/CascanteBonillaGroup/datasets/walking_tours/downloads/Iquitos_Peru - youtube video 0WpiskskL6Y [0WpiskskL6Y]/chunk_001.mp4\"\n",
    "    #video_path = \"/gpfs/projects/CascanteBonillaGroup/datasets/walking_tours/downloads/London_United Kingdom - LONDON 4K Walking Tour (UK) - 4h Tour with Captions Immersive Sound [4K Ultra HD60fps] [8WlUiln-VeY]/chunk_002.mp4\"\n",
    "    video_path = \"/gpfs/projects/CascanteBonillaGroup/datasets/walking_tours/downloads/Dubai_UAE - youtube video AjwcqYZ6cIw [AjwcqYZ6cIw]/chunk_001.mp4\"\n",
    "    \n",
    "    country = video_path.split('/')[7].split('_')[0] + '_' + video_path.split('/')[8].split('_')[1][:-4] \n",
    "    video_name = video_path.split('/')[7]\n",
    "    \n",
    "    video_metadata['path'] = video_path\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)#\"/gpfs/projects/CascanteBonillaGroup/datasets/walking_tours/downloads/Dubai_UAE - youtube video AjwcqYZ6cIw [AjwcqYZ6cIw]/chunk_001.mp4\")\n",
    "    \n",
    "    os.makedirs(video_name, exist_ok=True)\n",
    "    dir_path = video_name+'/video'\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps    = int(cap.get(cv2.CAP_PROP_FPS)) \n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    secs = total_frames // fps\n",
    "    video_metadata['fps'] = fps\n",
    "    video_metadata['original_total_frames'] = total_frames\n",
    "    video_metadata['original_secs'] = secs\n",
    "    total_frames = eval_frames#int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_metadata['total_frames'] = (total_frames // spacing) + 1\n",
    "    video_metadata['spacing'] = spacing\n",
    "    video_metadata['frames'] = []\n",
    "    \n",
    "    for count_frame in tqdm(range(total_frames), desc=\"Processing video\"):\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        if count_frame % spacing != 0:\n",
    "            continue\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Video ended, closing...\")\n",
    "            cap.release()\n",
    "            break\n",
    "    \n",
    "        cv2.imwrite(video_name+'/video/'+str(count_frame//spacing)+'.jpg', frame)\n",
    "    \n",
    "        results = model(frame, verbose=False)[0]  # Using predict() for latest ultralytics\n",
    "        detections = sv.Detections.from_ultralytics(results)\n",
    "        detections = detections[detections.confidence > 0.2]  # Filter low confidence\n",
    "        detections = detections[detections.class_id == 0]\n",
    "        tracks = tracker.update_with_detections(detections)#tracker.update(detections=detections)\n",
    "    \n",
    "        coord = np.zeros((1275, frame.shape[1],3)).astype(np.uint8)+255\n",
    "        edges = []\n",
    "        list_features = []\n",
    "        \n",
    "        c_frame = int(count_frame//spacing)\n",
    "        current_frame_info = {}    \n",
    "        current_frame_info['frame_id'] = c_frame\n",
    "        current_frame_info['detections'] = []\n",
    "    \n",
    "        for int_id, track_i in enumerate(tracks):\n",
    "            \n",
    "            detection = {}\n",
    "            track_id = int(track_i[4])\n",
    "            o1_x1, o1_y1, o1_x2, o1_y2 = map(int, track_i[0])\n",
    "            o1_mid = ((o1_x1+o1_x2)//2, (o1_y1+o1_y2)//2)\n",
    "            \n",
    "            opencv_frame = frame[int(o1_y1):int(o1_y2),int(o1_x1):int(o1_x2)]\n",
    "            pil_frame = opencv_to_pil(opencv_frame)\n",
    "            prob_male, answer_male = vqa_yes_prob(qwen, processor, pil_frame, 'Is the person a male?')\n",
    "            prob_female, answer_female = vqa_yes_prob(qwen, processor, pil_frame, 'Is the person a female?')\n",
    "            prob_child, answer_child = vqa_yes_prob(qwen, processor, pil_frame, 'Is the person a child?')\n",
    "            prob_nbin, answer_nbin = vqa_yes_prob(qwen, processor, pil_frame, 'Is the person non-binary?')\n",
    "    \n",
    "            person = np.array([prob_male, prob_female, prob_child, prob_nbin])\n",
    "            sex = 'unknown'\n",
    "            if np.argmax(person)   == 0:\n",
    "                sex='male'\n",
    "            elif np.argmax(person) == 1:\n",
    "                sex='female'\n",
    "            elif np.argmax(person) == 2:\n",
    "                sex='child'\n",
    "            elif np.argmax(person) == 3:\n",
    "                sex='non binary'\n",
    "                \n",
    "            pose_result = pose(opencv_frame)\n",
    "            direction, visible   = 'unknown', 'unknown'\n",
    "            \n",
    "            detection['body_parts'] = None\n",
    "            detection['conf_body_parts'] = None\n",
    "            detection['sex'] = sex\n",
    "            detection['track_id'] = track_id\n",
    "            \n",
    "            if len(pose_result[0].boxes) > 0:\n",
    "    \n",
    "                confs = pose_result[0].boxes.conf.cpu().numpy()\n",
    "                best_idx = confs.argmax()\n",
    "                \n",
    "                highest_conf_result = pose_result[0][best_idx:best_idx+1]            \n",
    "                confidence = highest_conf_result.keypoints.conf\n",
    "                \n",
    "                values = highest_conf_result.keypoints.conf>0.3\n",
    "                parts  = [\"Nose\", \"Left Eye\", \"Right Eye\", \"Left Ear\", \"Right Ear\", \"Left Shoulder\", \"Right Shoulder\",\"Left Elbow\",\"Right Elbow\",\"Left Wrist\",\"Right Wrist\",\"Left Hip\",\"Right Hip\",\"Left Knee\",\"Right Knee\",\"Left Ankle\",\"Right Ankle\"]\n",
    "                n_val = values.cpu().detach().numpy()[0]\n",
    "                key_values = highest_conf_result.keypoints.conf.cpu().detach().numpy()[0]\n",
    "                counts_points_body = np.sum(np.array(n_val[5:]))\n",
    "    \n",
    "                if counts_points_body:\n",
    "                    visible = 'not occluded'\n",
    "                else:\n",
    "                    visible = 'occluded'\n",
    "\n",
    "                if n_val[0] == True and n_val[1] == True and n_val[2] == True and n_val[3] == True and n_val[4] == True:\n",
    "                    direction = 'front'\n",
    "                elif n_val[0] == True and n_val[1] == True and n_val[2] == True and n_val[3] == True and n_val[4] == False:\n",
    "                    direction = 'front right'\n",
    "                elif n_val[0] == True and n_val[1] == True and n_val[2] == False and n_val[3] == True and n_val[4] == False:\n",
    "                    direction = 'front rright'\n",
    "                elif n_val[0] == True and n_val[1] == True and n_val[2] == True and n_val[3] == False and n_val[4] == True:\n",
    "                    direction = 'front left'\n",
    "                elif n_val[0] == True and n_val[1] == False and n_val[2] == True and n_val[3] == False and n_val[4] == True:\n",
    "                    direction = 'front lleft'\n",
    "                elif n_val[0] == False and n_val[1] == False and n_val[2] == False and n_val[3] == True and n_val[4] == True:\n",
    "                    direction = 'back'\n",
    "                elif n_val[0] == False and n_val[1] == False and n_val[2] == True and n_val[3] == True and n_val[4] == True:\n",
    "                    direction = 'back right'\n",
    "                elif n_val[0] == False and n_val[1] == True and n_val[2] == False and n_val[3] == True and n_val[4] == True:\n",
    "                    direction = 'back left'\n",
    "\n",
    "                detection['body_parts'] = parts\n",
    "                detection['conf_body_parts'] = key_values.tolist()\n",
    "            detection['direction'] = direction\n",
    "            detection['visible'] = visible\n",
    "            detection['bbox'] = [o1_x1, o1_y1, o1_x2, o1_y2]\n",
    "            current_frame_info['detections'].append(detection)\n",
    "        video_metadata['frames'].append(current_frame_info)\n",
    "    \n",
    "    with open(f\"{video_name}/{country}.json\", \"w\") as fp:\n",
    "        json.dump(video_metadata, fp, indent=4)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a17348-b756-4929-b48b-9274df646453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
