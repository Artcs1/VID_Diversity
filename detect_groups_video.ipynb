{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa51d744-56e7-4147-9691-3892feb50f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:19<00:00,  3.83s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Processing video:   0%|                                                                                                                                         | 0/100 [00:00<?, ?it/s]UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x256 1 person, 51.4ms\n",
      "Speed: 1.1ms preprocess, 51.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x192 4 persons, 51.6ms\n",
      "Speed: 1.1ms preprocess, 51.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x256 1 person, 10.9ms\n",
      "Speed: 1.1ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x160 (no detections), 51.1ms\n",
      "Speed: 0.6ms preprocess, 51.1ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x160 1 person, 10.2ms\n",
      "Speed: 0.6ms preprocess, 10.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x192 2 persons, 10.9ms\n",
      "Speed: 0.7ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x160 1 person, 10.9ms\n",
      "Speed: 0.9ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x224 1 person, 50.9ms\n",
      "Speed: 0.7ms preprocess, 50.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x288 1 person, 51.6ms\n",
      "Speed: 0.9ms preprocess, 51.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x288 (no detections), 10.5ms\n",
      "Speed: 0.9ms preprocess, 10.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 288)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   1%|█▎                                                                                                                               | 1/100 [00:17<29:22, 17.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x192 2 persons, 11.1ms\n",
      "Speed: 1.1ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x256 1 person, 10.9ms\n",
      "Speed: 1.2ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x256 1 person, 10.5ms\n",
      "Speed: 0.8ms preprocess, 10.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x224 1 person, 10.9ms\n",
      "Speed: 0.7ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x256 1 person, 10.6ms\n",
      "Speed: 0.8ms preprocess, 10.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x128 (no detections), 51.2ms\n",
      "Speed: 0.6ms preprocess, 51.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 128)\n",
      "\n",
      "0: 640x192 2 persons, 10.9ms\n",
      "Speed: 0.7ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  11%|██████████████                                                                                                                  | 11/100 [00:27<03:03,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x256 2 persons, 11.3ms\n",
      "Speed: 1.4ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x256 1 person, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x224 1 person, 10.8ms\n",
      "Speed: 2.0ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x192 2 persons, 11.2ms\n",
      "Speed: 1.1ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x224 1 person, 10.9ms\n",
      "Speed: 0.7ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x320 1 person, 52.1ms\n",
      "Speed: 1.0ms preprocess, 52.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x128 (no detections), 10.9ms\n",
      "Speed: 0.6ms preprocess, 10.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 128)\n",
      "\n",
      "0: 640x160 (no detections), 10.8ms\n",
      "Speed: 0.6ms preprocess, 10.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 160)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  21%|██████████████████████████▉                                                                                                     | 21/100 [00:38<02:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x256 1 person, 11.3ms\n",
      "Speed: 1.2ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x320 5 persons, 11.4ms\n",
      "Speed: 1.4ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x224 1 person, 10.9ms\n",
      "Speed: 0.8ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x224 2 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x224 1 person, 10.3ms\n",
      "Speed: 0.8ms preprocess, 10.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x256 1 person, 11.0ms\n",
      "Speed: 0.8ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  31%|███████████████████████████████████████▋                                                                                        | 31/100 [00:47<01:23,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x288 3 persons, 11.7ms\n",
      "Speed: 1.5ms preprocess, 11.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x224 1 person, 11.0ms\n",
      "Speed: 1.1ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x256 1 person, 10.8ms\n",
      "Speed: 1.1ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x192 2 persons, 11.3ms\n",
      "Speed: 1.1ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x224 1 person, 10.7ms\n",
      "Speed: 0.7ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x224 1 person, 10.1ms\n",
      "Speed: 0.7ms preprocess, 10.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  41%|████████████████████████████████████████████████████▍                                                                           | 41/100 [00:55<01:03,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x352 1 person, 52.7ms\n",
      "Speed: 1.6ms preprocess, 52.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 352)\n",
      "\n",
      "0: 640x256 1 person, 10.9ms\n",
      "Speed: 1.0ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x224 1 person, 11.1ms\n",
      "Speed: 1.0ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x224 1 person, 10.8ms\n",
      "Speed: 2.2ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x192 1 person, 10.8ms\n",
      "Speed: 0.7ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x192 1 person, 10.3ms\n",
      "Speed: 0.7ms preprocess, 10.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x256 1 person, 10.7ms\n",
      "Speed: 0.7ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  51%|█████████████████████████████████████████████████████████████████▎                                                              | 51/100 [01:05<00:51,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x224 1 person, 11.1ms\n",
      "Speed: 1.2ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x224 1 person, 10.9ms\n",
      "Speed: 1.1ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x288 1 person, 11.2ms\n",
      "Speed: 1.5ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x192 1 person, 11.0ms\n",
      "Speed: 0.9ms preprocess, 11.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x224 1 person, 11.1ms\n",
      "Speed: 0.8ms preprocess, 11.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x192 1 person, 10.8ms\n",
      "Speed: 0.7ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x288 1 person, 10.8ms\n",
      "Speed: 0.9ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 288)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  61%|██████████████████████████████████████████████████████████████████████████████                                                  | 61/100 [01:15<00:40,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x224 1 person, 11.2ms\n",
      "Speed: 1.2ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x224 2 persons, 10.6ms\n",
      "Speed: 1.3ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x224 1 person, 10.7ms\n",
      "Speed: 1.2ms preprocess, 10.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x192 2 persons, 10.9ms\n",
      "Speed: 0.7ms preprocess, 10.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x192 1 person, 10.5ms\n",
      "Speed: 0.7ms preprocess, 10.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x224 1 person, 10.7ms\n",
      "Speed: 0.8ms preprocess, 10.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x192 1 person, 11.4ms\n",
      "Speed: 0.7ms preprocess, 11.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x224 (no detections), 10.6ms\n",
      "Speed: 0.7ms preprocess, 10.6ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  71%|██████████████████████████████████████████████████████████████████████████████████████████▉                                     | 71/100 [01:26<00:30,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x224 2 persons, 10.7ms\n",
      "Speed: 1.2ms preprocess, 10.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x160 1 person, 11.3ms\n",
      "Speed: 0.7ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x320 1 person, 11.3ms\n",
      "Speed: 1.5ms preprocess, 11.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 320)\n",
      "\n",
      "0: 640x224 3 persons, 11.5ms\n",
      "Speed: 2.1ms preprocess, 11.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x192 1 person, 10.8ms\n",
      "Speed: 0.7ms preprocess, 10.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x224 (no detections), 10.7ms\n",
      "Speed: 0.8ms preprocess, 10.7ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  81%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                        | 81/100 [01:35<00:18,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x160 1 person, 11.1ms\n",
      "Speed: 0.9ms preprocess, 11.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x256 1 person, 11.2ms\n",
      "Speed: 0.9ms preprocess, 11.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x256 2 persons, 10.9ms\n",
      "Speed: 0.9ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x224 1 person, 10.6ms\n",
      "Speed: 0.8ms preprocess, 10.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x224 1 person, 10.8ms\n",
      "Speed: 1.3ms preprocess, 10.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 224)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:42<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import math\n",
    "import time\n",
    "import requests\n",
    "import torch\n",
    "import os\n",
    "import torchreid\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "import shutil\n",
    "import logging\n",
    "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from ultralytics import YOLO\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from our_utils import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    spacing = 10\n",
    "    eval_frames = 100\n",
    "    \n",
    "    video_metadata = {}\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    qwen = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\",torch_dtype=torch.bfloat16,attn_implementation=\"flash_attention_2\",device_map=\"cuda:0\",)\n",
    "    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "    qwen.eval()\n",
    "    \n",
    "    pose = YOLO(\"yolo11m-pose.pt\", 0.15)\n",
    "    model = YOLO(\"yolov10m.pt\")\n",
    "    tracker = sv.ByteTrack(track_activation_threshold=0.1)\n",
    "    palette = sv.ColorPalette.DEFAULT\n",
    "    \n",
    "    #cap = cv2.VideoCapture(\"input.mp4\")\n",
    "    #video_path = \"/gpfs/projects/CascanteBonillaGroup/datasets/walking_tours/downloads/Iquitos_Peru - youtube video 0WpiskskL6Y [0WpiskskL6Y]/chunk_001.mp4\"\n",
    "    #video_path = \"/gpfs/projects/CascanteBonillaGroup/datasets/walking_tours/downloads/London_United Kingdom - LONDON 4K Walking Tour (UK) - 4h Tour with Captions Immersive Sound [4K Ultra HD60fps] [8WlUiln-VeY]/chunk_002.mp4\"\n",
    "    video_path = \"/gpfs/projects/CascanteBonillaGroup/datasets/walking_tours/downloads/Dubai_UAE - youtube video AjwcqYZ6cIw [AjwcqYZ6cIw]/chunk_001.mp4\"\n",
    "    \n",
    "    country = video_path.split('/')[7].split('_')[0] + '_' + video_path.split('/')[8].split('_')[1][:-4] \n",
    "    video_name = video_path.split('/')[7]\n",
    "    \n",
    "    video_metadata['path'] = video_path\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)#\"/gpfs/projects/CascanteBonillaGroup/datasets/walking_tours/downloads/Dubai_UAE - youtube video AjwcqYZ6cIw [AjwcqYZ6cIw]/chunk_001.mp4\")\n",
    "    \n",
    "    os.makedirs(video_name, exist_ok=True)\n",
    "    dir_path = video_name+'/video'\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps    = int(cap.get(cv2.CAP_PROP_FPS)) \n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    secs = total_frames // fps\n",
    "    video_metadata['fps'] = fps\n",
    "    video_metadata['original_total_frames'] = total_frames\n",
    "    video_metadata['original_secs'] = secs\n",
    "    total_frames = eval_frames#int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_metadata['total_frames'] = (total_frames // spacing) + 1\n",
    "    video_metadata['spacing'] = spacing\n",
    "    video_metadata['frames'] = []\n",
    "    \n",
    "    for count_frame in tqdm(range(total_frames), desc=\"Processing video\"):\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        if count_frame % spacing != 0:\n",
    "            continue\n",
    "        \n",
    "        if not ret:\n",
    "            print(\"Video ended, closing...\")\n",
    "            cap.release()\n",
    "            break\n",
    "    \n",
    "        cv2.imwrite(video_name+'/video/'+str(count_frame//spacing)+'.jpg', frame)\n",
    "    \n",
    "        results = model(frame, verbose=False)[0]  # Using predict() for latest ultralytics\n",
    "        detections = sv.Detections.from_ultralytics(results)\n",
    "        detections = detections[detections.confidence > 0.2]  # Filter low confidence\n",
    "        detections = detections[detections.class_id == 0]\n",
    "        tracks = tracker.update_with_detections(detections)#tracker.update(detections=detections)\n",
    "    \n",
    "        coord = np.zeros((1275, frame.shape[1],3)).astype(np.uint8)+255\n",
    "        edges = []\n",
    "        list_features = []\n",
    "        \n",
    "        c_frame = int(count_frame//spacing)\n",
    "        current_frame_info = {}    \n",
    "        current_frame_info['frame_id'] = c_frame\n",
    "        current_frame_info['detections'] = []\n",
    "    \n",
    "        for int_id, track_i in enumerate(tracks):\n",
    "            \n",
    "            detection = {}\n",
    "            track_id = int(track_i[4])\n",
    "            o1_x1, o1_y1, o1_x2, o1_y2 = map(int, track_i[0])\n",
    "            o1_mid = ((o1_x1+o1_x2)//2, (o1_y1+o1_y2)//2)\n",
    "            \n",
    "            opencv_frame = frame[int(o1_y1):int(o1_y2),int(o1_x1):int(o1_x2)]\n",
    "            pil_frame = opencv_to_pil(opencv_frame)\n",
    "            prob_male, answer_male = vqa_yes_prob(qwen, processor, pil_frame, 'Is the person a male?')\n",
    "            prob_female, answer_female = vqa_yes_prob(qwen, processor, pil_frame, 'Is the person a female?')\n",
    "            prob_child, answer_child = vqa_yes_prob(qwen, processor, pil_frame, 'Is the person a child?')\n",
    "            prob_nbin, answer_nbin = vqa_yes_prob(qwen, processor, pil_frame, 'Is the person non-binary?')\n",
    "    \n",
    "            person = np.array([prob_male, prob_female, prob_child, prob_nbin])\n",
    "            sex = 'unknown'\n",
    "            if np.argmax(person)   == 0:\n",
    "                sex='male'\n",
    "            elif np.argmax(person) == 1:\n",
    "                sex='female'\n",
    "            elif np.argmax(person) == 2:\n",
    "                sex='child'\n",
    "            elif np.argmax(person) == 3:\n",
    "                sex='non binary'\n",
    "                \n",
    "            pose_result = pose(opencv_frame)\n",
    "            direction, visible   = 'unknown', 'unknown'\n",
    "            \n",
    "            detection['body_parts'] = None\n",
    "            detection['conf_body_parts'] = None\n",
    "            detection['sex'] = sex\n",
    "            detection['track_id'] = track_id\n",
    "            \n",
    "            if len(pose_result[0].boxes) > 0:\n",
    "    \n",
    "                confs = pose_result[0].boxes.conf.cpu().numpy()\n",
    "                best_idx = confs.argmax()\n",
    "                \n",
    "                highest_conf_result = pose_result[0][best_idx:best_idx+1]            \n",
    "                confidence = highest_conf_result.keypoints.conf\n",
    "                \n",
    "                values = highest_conf_result.keypoints.conf>0.3\n",
    "                parts  = [\"Nose\", \"Left Eye\", \"Right Eye\", \"Left Ear\", \"Right Ear\", \"Left Shoulder\", \"Right Shoulder\",\"Left Elbow\",\"Right Elbow\",\"Left Wrist\",\"Right Wrist\",\"Left Hip\",\"Right Hip\",\"Left Knee\",\"Right Knee\",\"Left Ankle\",\"Right Ankle\"]\n",
    "                n_val = values.cpu().detach().numpy()[0]\n",
    "                key_values = highest_conf_result.keypoints.conf.cpu().detach().numpy()[0]\n",
    "                counts_points_body = np.sum(np.array(n_val[5:]))\n",
    "    \n",
    "                if counts_points_body:\n",
    "                    visible = 'not occluded'\n",
    "                else:\n",
    "                    visible = 'occluded'\n",
    "\n",
    "                if n_val[0] == True and n_val[1] == True and n_val[2] == True and n_val[3] == True and n_val[4] == True:\n",
    "                    direction = 'front'\n",
    "                elif n_val[0] == True and n_val[1] == True and n_val[2] == True and n_val[3] == True and n_val[4] == False:\n",
    "                    direction = 'front right'\n",
    "                elif n_val[0] == True and n_val[1] == True and n_val[2] == False and n_val[3] == True and n_val[4] == False:\n",
    "                    direction = 'front rright'\n",
    "                elif n_val[0] == True and n_val[1] == True and n_val[2] == True and n_val[3] == False and n_val[4] == True:\n",
    "                    direction = 'front left'\n",
    "                elif n_val[0] == True and n_val[1] == False and n_val[2] == True and n_val[3] == False and n_val[4] == True:\n",
    "                    direction = 'front lleft'\n",
    "                elif n_val[0] == False and n_val[1] == False and n_val[2] == False and n_val[3] == True and n_val[4] == True:\n",
    "                    direction = 'back'\n",
    "                elif n_val[0] == False and n_val[1] == False and n_val[2] == True and n_val[3] == True and n_val[4] == True:\n",
    "                    direction = 'back right'\n",
    "                elif n_val[0] == False and n_val[1] == True and n_val[2] == False and n_val[3] == True and n_val[4] == True:\n",
    "                    direction = 'back left'\n",
    "\n",
    "                detection['body_parts'] = parts\n",
    "                detection['conf_body_parts'] = key_values.tolist()\n",
    "            detection['direction'] = direction\n",
    "            detection['visible'] = visible\n",
    "            detection['bbox'] = [o1_x1, o1_y1, o1_x2, o1_y2]\n",
    "            current_frame_info['detections'].append(detection)\n",
    "        video_metadata['frames'].append(current_frame_info)\n",
    "    \n",
    "    with open(f\"{video_name}/{country}.json\", \"w\") as fp:\n",
    "        json.dump(video_metadata, fp, indent=4)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a17348-b756-4929-b48b-9274df646453",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
